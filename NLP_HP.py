# -*- coding: utf-8 -*-
"""Copy of INFO490HP-P2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zXq9DFzv5U4rxudcropx80GJaak-Koq5
"""

# import nltk
# def do_downloads():
#   nltk.download('punkt')
#   nltk.download('averaged_perceptron_tagger')
#   nltk.download('maxent_ne_chunker')           
#   nltk.download('words')
#   nltk.download('stopwords') 
#   nltk.download('words')

# do_downloads()

# from nltk.corpus import stopwords

import urllib.request
import urllib.parse

def build_google_drive_url(doc_id):

  DRIVE1  = "https://docs.google.com/uc" 
  DRIVE2  = "https://drive.google.com/uc"  

  baseurl = DRIVE1 # DRIVE2 works as well 
  params = {"export" : "download",
            "id"     : doc_id}
 
  url = baseurl + "?" + urllib.parse.urlencode(params) 
  return url

def get_remote_text(url):
  try:
    with urllib.request.urlopen(url) as response:
      #status = response.getcode()
      #should check the status == 200 
      data = response.read()
      output = data.decode('utf-8')
      return output
  except urllib.error.HTTPError as e:
    # Return code error (e.g. 404, 501, ...)
    # ...
    print('HTTPError: {}'.format(e.code))
    return None
  except urllib.error.URLError as e:
    # Not an HTTP-specific error (e.g. connection refused)
    print('URLError: {}'.format(e.reason))
    return None

HP_SS_ID = '1jOCDUhsMY3uAoLqV3NoZogyrr-FVqEoo'

HP_url = build_google_drive_url(HP_SS_ID)

def get_harry_potter():
  return get_remote_text(HP_url)

hp = get_harry_potter()
print(len(hp))

import re

def clean_hp(text):
  start = text.find("\nHarry Potter and the Sorcerer's Stone\n")
  return (text[start:].strip())

import re
# import spacy

pronouns = ['they', 'your', "who", "she'd", "he'd", 'madam', 'he', "she", 'i', 'it', "i'm", "i've", "oh", "you", "mr", "mrs", "i'll", "i'd"]

def split_text_into_tokens(text):
  pattern = r"['A-Za-z0-9]+-?['A-Za-z0-9]+"
  regex   = re.compile(pattern) 
  long_tokens = regex.findall(text)
  strip_poss = [re.sub("'s", "", str) for str in long_tokens]
  strip_poss_v2 = [re.sub("'", "", str) for str in long_tokens]
  return strip_poss_v2


# nlp = spacy.load('en_core_web_sm')

# def load_stop_words(add_pronouns = False):
#   stop_words = list(nlp.Defaults.stop_words)
#   if add_pronouns == True:
#     return stop_words + pronouns
#   else:
#     return stop_words

# # from nltk.util import ngrams

# def bi_grams(tokens):
#   return ngrams(tokens, 2)


# import collections

# def top_n(tokens, n):
#   counter = collections.Counter(tokens)
#   return counter.most_common(n)


# def remove_stop_words(tokens, stoplist):
#   the_list = []
#   for word in tokens:
#     if word.lower() not in stoplist:
#       the_list.append(word)
#   return the_list


# def find_characters_v1(text, stoplist = [], top = 15):
#   tokenized = split_text_into_tokens(text)
#   strip_stop = remove_stop_words(tokenized, stoplist)
#   new_tokens = []
#   for word in tokenized:
#     if word[0].isupper():
#       new_tokens.append(word)
#   the_top_n = top_n(new_tokens,top)
#   return the_top_n

# def find_characters_v2(text, stoplist = [], top = 15):
#   tokenized = split_text_into_tokens(text)
#   the_bigrams = bi_grams(tokenized)
#   new_bigrams = []
#   for item in the_bigrams:
#     if item[0].istitle() and item[1].istitle():
#       if (item[0].lower() not in stoplist) and (item[1].lower() not in stoplist):
#         new_bigrams.append(item)
#   the_top_n = top_n(new_bigrams,top)
#   the_top_n_new = [((tup[0][0] + " " + tup[0][1]),tup[1]) for tup in the_top_n]
#   return the_top_n_new

# hp    = clean_hp(get_harry_potter())
# stop1 = load_stop_words(True)
# stop2 = load_stop_words()


# print(find_characters_v1(hp, stop1, 10))
# print(find_characters_v2(hp, stop2, 10))

# import nltk


# def nltk_find_people(text):
#   the_list = []
#   for sent in nltk.sent_tokenize(text):
#     tagged = nltk.pos_tag(nltk.word_tokenize(sent))
#     for chunk in nltk.ne_chunk(tagged):
#       if hasattr(chunk, 'label') and chunk.label() == 'PERSON':
#         name = ' '.join(c[0] for c in chunk)
#         the_list.append(name)
#   return the_list

# def find_characters_nlp(text, top = 15):
#   names = nltk_find_people(hp)
#   the_top_n = top_n(names,top)
#   return the_top_n

# print(find_characters_nlp(hp))

# import time

# start = time.time()
# print("hello")
# end = time.time()
# print(end - start)

def split_into_chapters(text):
  new_text = re.split("CHAPTER [A-Z]+\n", text)
  # new_text = re.split("CHAPTER [A-Z]+\n[A-Z\s'-]+\n", text)
  new_text_v2 = []
  for item in new_text:
    new_text_v2.append(item.strip())
  new_text_v2.remove(new_text_v2[0])
  return new_text_v2

# ex_text = '''

# Here is the front matter along with

# CHAPTER ONE
# HARRY POTTER IS A G

# and the ha

# CHAPTER TWO
# THIS IS-AND A

# bye gose
# '''

# print(hp[:5000])
# print(split_into_chapters(hp))

import numpy as np

def get_character_counts_v1(chapters):
  harry    = []
  for chapter in chapters:
    harry.append(np.char.count(chapter, "Harry"))
  ron    = []
  for chapter in chapters:
    ron.append(np.char.count(chapter, "Ron"))
  hagrid    = []
  for chapter in chapters:
    hagrid.append(np.char.count(chapter, "Hagrid"))
  hermione    = []
  for chapter in chapters:
    hermione.append(np.char.count(chapter, "Hermione"))
  return np.array([harry, ron, hagrid, hermione])

# print(get_character_counts_v1(split_into_chapters(hp)))
# print(split_into_chapters(hp)[6][:100])

import matplotlib.pyplot as plt

def simple_graph_v1(plots):
  fig = plt.figure()
  subplot = fig.add_subplot(1,1,1)
  subplot.plot(plots[0])
  subplot.plot(plots[1])
  subplot.plot(plots[2])
  subplot.plot(plots[3]) 
  # this is important for testing
  return fig

def pipeline_v1():
  hp = clean_hp(get_harry_potter())
  chapters = split_into_chapters(hp)
  plots = get_character_counts_v1(chapters)
  fig = simple_graph_v1(plots)
  return fig

pipeline_v1()

def get_character_counts_v2(chapters, names):
  py_data = [[np.char.count(chapter, name) for name in names] for chapter in chapters]
  counts = np.cumsum(np.array(py_data), axis = 0)
  counts_t = counts.T
  return counts_t.T

hp_chaps = split_into_chapters(hp)
who = ["Ron", "Hagrid"]
# print(get_character_counts_v2(hp_chaps, who))

ex = get_character_counts_v2(hp_chaps, who)
print(ex.shape[0])

# print(get_character_counts_v1(split_into_chapters(hp)))
# print(split_into_chapters(hp)[6][:100])

def simple_graph_v2(counts):
  fig = plt.figure()
  subplot = fig.add_subplot(1,1,1)
  subplot.plot(counts)
  return fig  # return the figure

# test it
# who = ["Harry", "Ron", "Hagrid", "Hermione"]
# data = pipeline_v2(who)
# simple_graph_v2(data)



def simple_graph_hp(counts, names):
  fig, axes = plt.subplots(nrows=1,ncols=1)
  axes.plot(counts)

  # fig = plt.figure()
  # subplot = fig.add_subplot(1,1,1)
  # subplot.plot(counts)

  fig.suptitle('HP Characters LAP')
  # axes.set_title('HP Characters LAP', fontsize=12)
  axes.legend(labels=names, loc='upper left')

  axes.grid(True)

  axes.set_xlabel('Chapters')
  axes.set_ylabel('Cumulative Count')

  x_count = int(np.ma.size(counts, axis = 0))
  x_min = int(np.amin(np.arange(x_count)))
  x_max = int(np.amax(np.arange(x_count)))
  # print(chap_range, x_count, x_min, x_max)
  ticks = np.linspace(int(x_min), int(x_count), int(x_count + 1))
  ticks = [int(num) for num in ticks]
  labels = ["{}".format(i) for i in ticks]
  axes.set_xticks(ticks)
  axes.set_xticklabels(labels, fontsize=10)


  return fig

plt.style.use('fivethirtyeight')

def pipeline_v2(names):

  hp = clean_hp(get_harry_potter())
  chapters = split_into_chapters(hp)
 
  np_hp = get_character_counts_v2(chapters, names)
  
  fig = simple_graph_hp(np_hp, names)
  return fig

who = ["Harry", "Ron", "Hagrid", "Hermione"]
fig = pipeline_v2(who)